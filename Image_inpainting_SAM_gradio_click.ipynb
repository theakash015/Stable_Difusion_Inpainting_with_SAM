{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y3srqAlXjq8",
        "outputId": "704b9510-c70c-4c2c-d7b1-6ae078ed1e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q regex tqdm\n",
        "!pip install -q diffusers transformers accelerate scipy\n",
        "!pip install -q -U xformers==0.0.25\n",
        "!pip install -q opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjzOhEOQbm8M"
      },
      "outputs": [],
      "source": [
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0ExjvOTXvG-"
      },
      "outputs": [],
      "source": [
        "!pip install -q pycocotools matplotlib onnxruntime onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exLx9ezpdILK"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtlPtucLXxnd"
      },
      "outputs": [],
      "source": [
        "### Import libraries\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
        "\n",
        "import PIL, cv2\n",
        "from PIL import Image\n",
        "\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "import base64, json, requests\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "from numpy import asarray\n",
        "\n",
        "import sys\n",
        "\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIAKThm4YEDW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from diffusers import StableDiffusionInpaintPipeline, EulerDiscreteScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiM8xJrCX5Ge"
      },
      "outputs": [],
      "source": [
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN6k972Oj3YE"
      },
      "outputs": [],
      "source": [
        "model_type='vit_h'\n",
        "device='cuda'\n",
        "sam_checkpoint='sam_vit_h_4b8939.pth'\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "  # Pay attention to the pred_iou_thresh, the lower the more masks it will generate. Make it higher to generate masks only of\n",
        "  # good quality and less number of them\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,\n",
        "    pred_iou_thresh=0.97, # A filtering threshold in [0,1], using the model's predicted mask quality. iou -> intersection over union process which is used for checking quality of Segmentation process.\n",
        "    stability_score_thresh=0.92,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPttfwuNX8Lt"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionInpaintPipeline, EulerDiscreteScheduler #Inpainting pipeline of Stable diffusion\n",
        "# The Euler discrete Scheduler is a specific type of scheduler. When we are running a diffusion generative model in inference, we are gonna start with a image with noise/noise\n",
        "# and we are going to go gradually back to a full image.(Denoising) That gradual process can be done in various ways. Euler Discrete SCheduler is one of them.\n",
        "\n",
        "model_dir=\"stabilityai/stable-diffusion-2-inpainting\"   # mention the type of the stable diffusion you want.\n",
        "\n",
        "### The scheduler determines the algorithm used to produce new samples during the denoising process\n",
        "scheduler = EulerDiscreteScheduler.from_pretrained(model_dir, subfolder=\"scheduler\")\n",
        "### pipeline\n",
        "pipe = StableDiffusionInpaintPipeline.from_pretrained(model_dir,\n",
        "                                                   scheduler=scheduler,\n",
        "                                                   revision=\"fp16\", #run the model is floating ploat 16 bit precision\n",
        "                                                   torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "pipe.enable_xformers_memory_efficient_attention() #X formers makes the execution of the transformer more efficient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAM04j0aECuK"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def show_anns(anns):\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    centroids={}\n",
        "    # Sort masks by area in descending order\n",
        "    sorted_anns = sorted(enumerate(anns), key=(lambda x: x[1]['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    # Disable autoscale to keep the image size consistent\n",
        "    ax.set_autoscale_on(False)\n",
        "\n",
        "    # Iterate through each mask and display it on top of the original image\n",
        "    for original_idx, ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
        "\n",
        "        # Generate a random color for the mask\n",
        "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
        "        for i in range(3):\n",
        "            img[:,:,i] = color_mask[i]\n",
        "\n",
        "        # Blend the mask with the image, using 0.35 as the alpha value for transparency\n",
        "        ax.imshow(np.dstack((img, m*0.35)))\n",
        "\n",
        "        # Find contours of the mask to compute the centroid\n",
        "        contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if contours:\n",
        "            cnt = contours[0]\n",
        "            M = cv2.moments(cnt)\n",
        "\n",
        "            # Compute the centroid of the mask if the moment is non-zero\n",
        "            if M[\"m00\"] != 0:\n",
        "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
        "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
        "                centroids[original_idx] = (cx, cy)\n",
        "                # Plot a marker at the centroid with a star shape\n",
        "                ax.plot(cx, cy, marker='.', color='white', markersize=10)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW17EUIrpqp8"
      },
      "outputs": [],
      "source": [
        "def find_bounding_box(image_array):\n",
        "    # Find indices of non-white pixels\n",
        "    non_white_indices = np.where(np.any(image_array[..., :3] != 255, axis=-1))\n",
        "    # Calculate bounding box coordinates\n",
        "    min_y, min_x = np.min(non_white_indices, axis=1)\n",
        "    max_y, max_x = np.max(non_white_indices, axis=1)\n",
        "    return min_x, min_y, max_x, max_y\n",
        "\n",
        "def sam_model(img):\n",
        "\n",
        "    seg = asarray(img)\n",
        "    masks = mask_generator.generate(seg)\n",
        "\n",
        "    # Display the original image with annotations\n",
        "    plt.imshow(img)\n",
        "    c=show_anns(masks)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Render the figure and convert it to a PIL image\n",
        "    plt_img = plt.gcf()\n",
        "    plt_img.canvas.draw()\n",
        "    image_array = np.array(plt_img.canvas.renderer._renderer)\n",
        "\n",
        "    # Convert RGBA image to RGB\n",
        "    if image_array.shape[-1] == 4:\n",
        "        image_array = image_array[..., :3]\n",
        "\n",
        "    pil_image = Image.fromarray(image_array)\n",
        "\n",
        "    # Resize the image to match the original PIL image size\n",
        "    pil_image = pil_image.resize(img.size)  # Assuming img is a PIL image\n",
        "\n",
        "    # Convert the resized PIL image back to a NumPy array\n",
        "    image_array_resized = np.array(pil_image)\n",
        "\n",
        "    # Find bounding box of non-white pixels\n",
        "    min_x, min_y, max_x, max_y = find_bounding_box(image_array_resized)\n",
        "\n",
        "    # Crop the resized image using the bounding box coordinates\n",
        "    cropped_image_array = image_array_resized[min_y:max_y, min_x:max_x]\n",
        "\n",
        "    # Resize the cropped image to match the size of the original image\n",
        "    cropped_pil_image = Image.fromarray(cropped_image_array)\n",
        "    cropped_pil_image = cropped_pil_image.resize(img.size)\n",
        "\n",
        "    return cropped_pil_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSjql-Vk-tNz"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "def on_select(masked_image, source_image,evt: gr.SelectData):\n",
        "    input_points = np.array(evt.index)\n",
        "    formatted_points = ','.join(map(str, input_points))\n",
        "    formatted_points = '[' + formatted_points + ']'\n",
        "    # print(formatted_points)\n",
        "    seg = asarray(source_image)\n",
        "    masks = mask_generator.generate(seg)\n",
        "    c=show_anns(masks)\n",
        "\n",
        "    min_dist = float('inf')\n",
        "    nearest_index = None\n",
        "\n",
        "    for index, coord in c.items():\n",
        "        dist = np.linalg.norm(np.array(coord) - np.array(input_points))\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            nearest_index = index\n",
        "\n",
        "    segmentation_mask=masks[nearest_index]['segmentation']\n",
        "    stable_diffusion_mask=PIL.Image.fromarray(segmentation_mask)\n",
        "    return stable_diffusion_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2aIaxOJyLw5"
      },
      "outputs": [],
      "source": [
        "def mask_final(source_image,stable_diffusion_mask,inpainting_prompt):\n",
        "  generator = torch.Generator(device=\"cuda\").manual_seed(155)\n",
        "  image = pipe(prompt=inpainting_prompt, guidance_scale=30, num_inference_steps=150, generator=generator, image=source_image, mask_image=stable_diffusion_mask).images[0]\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 630,
          "referenced_widgets": [
            "59f8a2ef0d624cdc8ecb4b298a290af5"
          ]
        },
        "id": "DGivP6FczSR5",
        "outputId": "38eefcb4-8209-4b35-ae60-5dd7447252fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f02a2aa09eb56ea4fb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f02a2aa09eb56ea4fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59f8a2ef0d624cdc8ecb4b298a290af5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/150 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f02a2aa09eb56ea4fb.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#final code--just index improvment\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  gr.Markdown(\"# Stable Diffusion with Segment Anything!\")\n",
        "  with gr.Row():\n",
        "      with gr.Column():\n",
        "          image = gr.Image(type='pil')\n",
        "      with gr.Column():\n",
        "          masked_image = gr.Image(type='pil')\n",
        "\n",
        "  with gr.Row():\n",
        "      with gr.Column():\n",
        "          mask=gr.Image(type='pil')\n",
        "\n",
        "      with gr.Column():\n",
        "        prompt = gr.Textbox(placeholder=\"Processing Prompt\",label='Prompt')\n",
        "\n",
        "  with gr.Row():\n",
        "    button_final = gr.Button(\"Process Image\")\n",
        "\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "          output = gr.Image(type='pil')\n",
        "\n",
        "  image.change(sam_model, inputs=image, outputs=masked_image)\n",
        "  masked_image.select(on_select, inputs=[masked_image,image],outputs=mask)\n",
        "  button_final.click(mask_final, inputs=[image,mask,prompt],outputs=output)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjrVyjL5BMg0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}